# ecomm_datapipeline
An ELT (Extract, Load, Transform) data pipeline built on Google Cloud Platform for processing e-commerce order data in real-time and generating business intelligence insights.
## ğŸ¯ Project Overview

This project demonstrates end-to-end data engineering capabilities by building an automated pipeline that:
- Processes incoming order data every 15 minutes
- Maintains a star schema data warehouse in BigQuery
- Performs automated data quality checks
- Generates real-time analytics and daily business reports
- Orchestrates complex workflows using Apache Airflow

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Sources                             â”‚
â”‚  â€¢ Orders (JSON) - Every 15 minutes                         â”‚
â”‚  â€¢ Products (CSV) - Daily updates                           â”‚
â”‚  â€¢ Customers (CSV) - Daily updates                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cloud Storage (GCS)                            â”‚
â”‚  Landing Zone â†’ Staging â†’ Archive                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Cloud Composer (Apache Airflow)                    â”‚
â”‚  â€¢ Incremental Pipeline (15 min)                            â”‚
â”‚  â€¢ Daily Batch Jobs                                         â”‚
â”‚  â€¢ Data Quality Checks                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BigQuery Data Warehouse                    â”‚
â”‚                                                             â”‚
â”‚  Staging Layer:                                             â”‚
â”‚  â”œâ”€â”€ staging_orders                                         â”‚
â”‚  â”œâ”€â”€ staging_products                                       â”‚
â”‚  â””â”€â”€ staging_customers                                      â”‚
â”‚                                                             â”‚
â”‚  Core Layer (Star Schema):                                  â”‚
â”‚  â”œâ”€â”€ fact_orders (Fact Table)                               â”‚
â”‚  â”œâ”€â”€ dim_products (Dimension)                               â”‚
â”‚  â”œâ”€â”€ dim_customers (Dimension)                              â”‚
â”‚  â””â”€â”€ agg_hourly_metrics (Aggregation)                       â”‚
â”‚                                                             â”‚
â”‚  Data Quality Layer:                                        â”‚
â”‚  â””â”€â”€ data_quality_checks                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|-----------|
| **Cloud Platform** | Google Cloud Platform (GCP) |
| **Orchestration** | Apache Airflow (Cloud Composer) |
| **Data Warehouse** | BigQuery |
| **Data Lake** | Cloud Storage (GCS) |
| **Programming** | Python 3.11, SQL |
| **Data Processing** | BigQuery SQL, Airflow Operators |

## ğŸ“Š Data Model

### Star Schema Design

**Fact Table:**
- `fact_orders` - Grain: One row per product per order
  - Captures: order details, customer info, product info, pricing, location

**Dimension Tables:**
- `dim_customers` - Customer master data with tier information
- `dim_products` - Product catalog with pricing and inventory

**Aggregation Tables:**
- `agg_hourly_metrics` - Pre-aggregated metrics for dashboard performance
- `agg_category_revenue` - Revenue metrics by product category

## ğŸ”„ Pipeline Workflows

### 1. Incremental Order Processing (Every 15 Minutes)

**DAG: `order_processing_incremental`**

```
1. Sensor Task â†’ Detect new order files in GCS
2. Load Task â†’ Load JSON to staging_orders
3. Quality Checks â†’ 
   â”œâ”€â”€ Check for duplicate orders
   â””â”€â”€ Validate order amounts
4. Transform Task â†’ Flatten & enrich data to fact_orders
5. Aggregation Task â†’ Update hourly metrics
6. Archive Task â†’ Move processed files to archive
```

**Key Features:**
- Idempotent processing (prevents duplicate processing)
- Automatic retry on failure (2 retries with 5-min delay)
- Partitioned by date, clustered by customer_id for query performance

### 2. Daily Batch Processing (Midnight)

**DAG: `daily_batch_processing`**

```
1. Load dimensions â†’ Update product & customer data
2. Generate Reports â†’
   â”œâ”€â”€ Inactive customers (30+ days no orders)
   â”œâ”€â”€ Low stock alerts (high demand + low inventory)
   â””â”€â”€ Revenue trends by region
3. Data Quality Audit â†’ Summary statistics
```

## ğŸ¯ Data Quality Framework

Automated checks at every stage:

| Check Type | Description | Action |
|------------|-------------|--------|
| **Duplicate Detection** | Identifies duplicate order_id | Fail pipeline |
| **Amount Validation** | Verifies total = sum(item prices) | Log warning |
| **Schema Validation** | Ensures required fields present | Fail task |
| **Missing References** | Checks customer/product exists | Log to audit table |

All quality issues logged to `data_quality_checks` table with severity levels.

## ğŸ“ˆ Business Intelligence Outputs

### Real-Time Dashboard Metrics (15-min refresh)
- Orders per hour
- Revenue by product category
- Top 10 selling products
- Average order value by customer tier
- Geographic distribution

### Daily Reports
1. **Customer Churn Risk** - Customers inactive 30+ days
2. **Inventory Alerts** - Products with high sales but low stock
3. **Revenue Trends** - 90-day trends by region and category

## ğŸš€ Setup Instructions

### Prerequisites
- GCP Account with billing enabled
- `gcloud` CLI installed and configured
- Python 3.12

### 1. Clone Repository
```bash
git clone https://github.com/zackkki/ecomm_datapipeline.git
cd ecommerce-data-pipeline
```

### 2. Set Up GCP Resources

```bash
# Set project
export PROJECT_ID="your-project-id"
gcloud config set project $PROJECT_ID

# Create GCS bucket
gsutil mb -l us-central1 gs://${PROJECT_ID}-data-pipeline

# Create bucket structure
gsutil mkdir gs://${PROJECT_ID}-data-pipeline/landing/orders
gsutil mkdir gs://${PROJECT_ID}-data-pipeline/landing/products
gsutil mkdir gs://${PROJECT_ID}-data-pipeline/landing/customers
gsutil mkdir gs://${PROJECT_ID}-data-pipeline/archive

# Create BigQuery dataset
bq mk --location=asia-southeast1 ecommerce_data
```

### 3. Create BigQuery Tables

```bash
# Run the schema creation script
bq query --use_legacy_sql=false < sql/create_tables.sql
```

### 4. Set Up Cloud Composer

```bash
# Create Composer environment
gcloud composer environments create ecommerce-pipeline \
    --location asia-southeast1 \
    --image-version composer-3-airflow-2.10.5 \

# Get DAGs folder
export DAGS_FOLDER=$(gcloud composer environments describe ecommerce-pipeline \
    --location asia-southeast1 \
    --format="get(config.dagGcsPrefix)")
```

### 5. Deploy Airflow DAGs

```bash
# Upload DAGs
gsutil cp dags/*.py $DAGS_FOLDER/

# Grant permissions to Composer service account
export COMPOSER_SA=$(gcloud composer environments describe ecommerce-pipeline \
    --location asia-southeast1 \
    --format="get(config.nodeConfig.serviceAccount)")

gsutil iam ch serviceAccount:${COMPOSER_SA}:objectAdmin \
    gs://${PROJECT_ID}-data-pipeline
```

### 6. Generate and Upload Sample Data

```bash
# Generate sample data
python scripts/generate_orders.py
python scripts/generate_products.py
python scripts/generate_customers.py

# Upload to GCS
gsutil cp data/orders_sample.json \
    gs://${PROJECT_ID}-data-pipeline/landing/orders/orders_$(date +%Y%m%d_%H%M%S).json
gsutil cp data/products_sample.csv \
    gs://${PROJECT_ID}-data-pipeline/landing/products/products_latest.csv
gsutil cp data/customers_sample.csv \
    gs://${PROJECT_ID}-data-pipeline/landing/customers/customers_latest.csv
```

### 7. Enable DAGs in Airflow UI

1. Access Airflow UI:
```bash
gcloud composer environments describe ecommerce-pipeline \
    --location asia-southeast1 \
    --format="get(config.airflowUri)"
```

2. Toggle ON the DAGs:
   - `order_processing_incremental`
   - `daily_batch_processing` (create this based on requirements)

3. Manually trigger for testing

## ğŸ“ Project Structure

```
ecommerce-datapipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ order_processing_incremental.py    # 15-min incremental pipeline
â”‚   
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_orders.py                 # Sample data generator
â”‚   â”œâ”€â”€ generate_products.py
â”‚   â””â”€â”€ generate_customers.py
â””â”€â”€ README.md


## ğŸ“Š Monitoring & Observability

- **Airflow UI**: Task status, logs, execution times
- **BigQuery Audit Logs**: Query performance, slot usage
- **Cloud Monitoring**: Set up alerts for:
  - Pipeline failures
  - Data quality check failures
  - Processing time anomalies
  - Cost thresholds

## ğŸ“ Key Learnings & Design Decisions

### Why ELT over ETL?
- Leverages BigQuery's distributed processing power
- Keeps raw data intact for reprocessing
- Enables faster iteration on transformations
- Scales better with data volume growth

### Why Star Schema?
- Optimized for analytical queries
- Denormalized structure improves query performance
- Intuitive for business users
- Supports both detailed and aggregated analysis

### Why 15-Minute Increments?
- Balances freshness with cost
- Reduces API calls vs real-time streaming
- Allows batch processing optimizations
- Sufficient for business decision-making

## ğŸ”® Future Enhancements

- [ ] Add dbt for transformation layer management
- [ ] Implement CDC (Change Data Capture) for real-time updates
- [ ] Add data lineage tracking with Apache Atlas
- [ ] Create Looker/Data Studio dashboards
- [ ] Implement data versioning and time travel
- [ ] Add machine learning predictions (demand forecasting)
- [ ] Set up automated data profiling with Great Expectations

## ğŸ“ License

MIT License - feel free to use this project for learning and portfolio purposes.

## ğŸ™ Acknowledgments

- Built as part of data engineering learning journey
- Inspired by real-world e-commerce analytics requirements
- Thanks to the open-source community for amazing tools

---

**â­ If you find this project helpful, please give it a star!**
